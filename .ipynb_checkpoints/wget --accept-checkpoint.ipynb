{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wget using accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os, csv\n",
    "import shutil\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "from socket import error as SocketError\n",
    "import errno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setting directories\n",
    "micro_sample_cvs = \"/Users/anhnguyen/Desktop/research/scraping_Python/micro-sample_Feb17.csv\"\n",
    "wget_folder = \"/Users/anhnguyen/Desktop/research/scraping_Python/wget_accept\"\n",
    "no_dir_folder = \"/Users/anhnguyen/Desktop/research/scraping_Python/no_dir\"\n",
    "learning_wget = \"/Users/anhnguyen/Desktop/research/scraping_Python/learning_wget\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = [] # make empty list\n",
    "with open(micro_sample_cvs, 'r', encoding = 'Windows-1252')\\\n",
    "as csvfile: # open file; the windows-1252 encoding looks weird but works for this\n",
    "    reader = csv.DictReader(csvfile) # create a reader\n",
    "    for row in reader: # loop through rows\n",
    "        sample.append(row) # append each row to the list\n",
    "        \n",
    "#note: each row, sample[i] is a dictionary with keys as column name and value as info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turning this into tuples we can use with wget!\n",
    "# first, make some empty lists\n",
    "url_list = []\n",
    "name_list = []\n",
    "terms_list = []\n",
    "\n",
    "# now let's fill these lists with content from the sample\n",
    "for school in sample:\n",
    "    url_list.append(school[\"URL\"])\n",
    "    name_list.append(school[\"SCHNAM\"])\n",
    "    terms_list.append(school[\"ADDRESS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://www.richland2.org/charterhigh/', 'RICHLAND TWO CHARTER HIGH'), ('https://www.polk.edu/lakeland-gateway-to-college-high-school/', 'POLK STATE COLLEGE COLLEGIATE HIGH SCHOOL'), ('https://www.nhaschools.com/schools/rivercity/Pages/default.aspx', 'RIVER CITY SCHOLARS CHARTER ACADEMY')]\n",
      "\n",
      " Polk State College Collegiate High School\n"
     ]
    }
   ],
   "source": [
    "tuple_list = list(zip(url_list, name_list))\n",
    "# Let's check what these tuples look like:\n",
    "print(tuple_list[:3])\n",
    "print(\"\\n\", tuple_list[1][1].title())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def format_folder_name (k, name):\n",
    "    \"\"\"Format a folder nicely for easy access\"\"\"\n",
    "    if k < 10: # Add two zeros to the folder name if k is less than 10 (for ease of organizing the output folders)\n",
    "        dirname = \"00\" + str(k) + \" \" + name\n",
    "    elif k < 100: # Add one zero if k is less than 100\n",
    "        dirname = \"0\" + str(k) + \" \" + name\n",
    "    else: # Add nothing if k>100\n",
    "        dirname = str(k) + \" \" + name\n",
    "    return dirname\n",
    "\n",
    "def run_wget_command(link, parent_folder, my_folder):\n",
    "    \"\"\"wget on link and print output to appropriate folders\"\"\"\n",
    "    #navigate to parent folder\n",
    "    os.chdir(parent_folder)\n",
    "    # create dir my_folder if it doesn't exist yet\n",
    "    if not os.path.exists(my_folder):\n",
    "        os.makedirs(my_folder)\n",
    "    #navigate to the correct folder, ready to wget\n",
    "    os.chdir(my_folder)\n",
    "    \n",
    "    os.system('wget -np --no-parent --show-progress --progress=dot --recursive --level=3 --convert-links --retry-connrefused \\\n",
    "         --random-wait --no-cookies --secure-protocol=auto --no-check-certificate --execute robots=off \\\n",
    "         --header \"Host: jrs-s.net\" \\\n",
    "         --user-agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\" \\\n",
    "          --accept .html' + ' ' + link)\n",
    "    \n",
    "\n",
    "def contains_html(my_folder):\n",
    "    \"\"\"check if a wget is success by checking if a directory has a html file\"\"\"\n",
    "\n",
    "    for r,d,f in os.walk(my_folder):\n",
    "        for file in f:\n",
    "            if file.endswith('.html'):\n",
    "                return True\n",
    "    return False  \n",
    "def count_with_file_ext(folder, ext):\n",
    "    count = 0\n",
    "    for r,d,f in os.walk(my_folder):\n",
    "        for file in f:\n",
    "            if file.endswith(ext):\n",
    "                count +=1\n",
    "    return count  \n",
    "def write_to_file(num, link, file_name):\n",
    "    with open(file_name, \"a\") as text_file:\n",
    "        text_file.write(str(num) + \"\\t\" + link +\"\\n\")\n",
    "        \n",
    "def write_file(str, file_name):\n",
    "    with open(file_name, \"a\") as text_file:\n",
    "        text_file.write(str)\n",
    "        \n",
    "def reset(folder, text_file_1, text_file_2):\n",
    "    \"\"\"Deletes all files in a folder and set 2 text files to blank\"\"\"\n",
    "    parent_folder = folder[: folder.rindex('/')]\n",
    "    shutil.rmtree(folder)\n",
    "    os.makedirs(folder)\n",
    "    filelist = [ f for f in os.listdir(folder) if f.endswith(\".bak\") ]\n",
    "    for f in filelist:\n",
    "        os.unlink(f)\n",
    "    for file_name in [text_file_1, text_file_2]:\n",
    "        reset_text_file(file_name)\n",
    "        \n",
    "def reset_text_file(file_name):\n",
    "    if os.path.exists(file_name):\n",
    "            with open(file_name, \"w\") as text_file:\n",
    "                text_file.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "030 name me\n"
     ]
    }
   ],
   "source": [
    "#testing methods\n",
    "print(format_folder_name(30, \"name me\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up file directories\n",
    "success_file = \"/Users/anhnguyen/Desktop/research/scraping_Python/success.txt\"\n",
    "fail_file = \"/Users/anhnguyen/Desktop/research/scraping_Python/fail.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reset(wget_folder, success_file, fail_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing website data for City On A Hill Charter Public School Ii, which is school #201 of 300...\n",
      "Capturing website data for Circles Of Success Learning Academy, which is school #202 of 300...\n",
      "Capturing website data for Chicago Park Community Charter, which is school #203 of 300...\n",
      "Capturing website data for Chandler Park Academy - Elementary, which is school #204 of 300...\n",
      "Capturing website data for Cedar Tree Academy Pcs, which is school #205 of 300...\n",
      "Capturing website data for Capital City Lower Pcs, which is school #206 of 300...\n",
      "Capturing website data for Cardinal Community Academy Charter School, which is school #207 of 300...\n",
      "Capturing website data for Camino Nuevo Charter Academy No. 4, which is school #208 of 300...\n",
      "Capturing website data for Camarillo Academy Of Progressive Education, which is school #209 of 300...\n",
      "Capturing website data for Bridgewater Academy Charter, which is school #210 of 300...\n",
      "Capturing website data for The James And Grace Lee Boggs School, which is school #211 of 300...\n",
      "Capturing website data for Banning Lewis Ranch Academy, which is school #212 of 300...\n",
      "Capturing website data for The Bloomington Project School, which is school #213 of 300...\n",
      "Capturing website data for Bay Haven Charter Academy, which is school #214 of 300...\n",
      "Capturing website data for Ben Gamla Charter School South Broward, which is school #215 of 300...\n",
      "Capturing website data for Belle Creek Charter School, which is school #216 of 300...\n",
      "Capturing website data for Bucks County Montessori Cs, which is school #217 of 300...\n",
      "Capturing website data for Blackfoot Charter Community, which is school #218 of 300...\n",
      "Capturing website data for Bayshore Preparatory Charter, which is school #219 of 300...\n",
      "Capturing website data for Basis Tucson, which is school #220 of 300...\n",
      "Capturing website data for City Springs Elementary, which is school #221 of 300...\n",
      "Capturing website data for Academy For Science And Design Charter (M), which is school #222 of 300...\n",
      "Capturing website data for St. Johns Community Campus, which is school #223 of 300...\n",
      "Capturing website data for Appleton Public Montessori, which is school #224 of 300...\n",
      "Capturing website data for Academy For Math Engineering & Science (Ames), which is school #225 of 300...\n",
      "Capturing website data for Alta Vista Charter School, which is school #226 of 300...\n",
      "Capturing website data for Alpha International Academy, which is school #227 of 300...\n",
      "Capturing website data for All Tribes Charter, which is school #228 of 300...\n",
      "Capturing website data for Academic And Career Education Academy, which is school #229 of 300...\n",
      "Capturing website data for Academy Of Tucson Middle School, which is school #230 of 300...\n",
      "Capturing website data for Academy Charter High School, which is school #231 of 300...\n",
      "Capturing website data for La Academia De Estrellas, which is school #232 of 300...\n",
      "Capturing website data for Academy Of Arts And Sciences: Oxnard & Ventura, which is school #233 of 300...\n",
      "Capturing website data for Millennium Community School, which is school #234 of 300...\n",
      "Capturing website data for Westar Elementary School, which is school #235 of 300...\n",
      "Capturing website data for Walatowa Charter High, which is school #236 of 300...\n",
      "Capturing website data for Magnolia Science Academy 7, which is school #237 of 300...\n",
      "Capturing website data for Trio Wolf Creek Distance Learning, which is school #238 of 300...\n",
      "Capturing website data for Broadway Academy, which is school #239 of 300...\n",
      "Capturing website data for The Classical Academy High School, which is school #240 of 300...\n",
      "Capturing website data for Stamford Academy, which is school #241 of 300...\n",
      "Capturing website data for Somerset Academy-Middle, Eagle Campus, which is school #242 of 300...\n",
      "Capturing website data for Arroyo Vista Charter, which is school #243 of 300...\n",
      "Capturing website data for Rincon Vista Middle School, which is school #244 of 300...\n",
      "Capturing website data for Pittman Charter, which is school #245 of 300...\n",
      "Capturing website data for Premier H S Of Pharr, which is school #246 of 300...\n",
      "Capturing website data for Premier H S Of New Braunfels, which is school #247 of 300...\n",
      "Capturing website data for Vista Academy Of Huntsville, which is school #248 of 300...\n",
      "Capturing website data for Vista Academy Of Dallas, which is school #249 of 300...\n",
      "Capturing website data for Roosevelt Children'S Academy Charter School, which is school #250 of 300...\n",
      "Capturing website data for Sand Ridge Charter School, which is school #251 of 300...\n",
      "Capturing website data for Pacific Heritage Academy, which is school #252 of 300...\n",
      "Capturing website data for Reid Traditional Schools' Painted Rock Academy, which is school #253 of 300...\n",
      "Capturing website data for Legacy Traditional School - Northwest Tucson, which is school #254 of 300...\n",
      "Capturing website data for Mott Hall Charter School, which is school #255 of 300...\n",
      "Capturing website data for Mid-Michigan Leadership Academy, which is school #256 of 300...\n",
      "Capturing website data for Marshall Lane Elementary, which is school #257 of 300...\n",
      "Capturing website data for Manitowoc County Comprehensive Charter School, which is school #258 of 300...\n",
      "Capturing website data for Longleaf School Of The Arts, which is school #259 of 300...\n",
      "Capturing website data for Lighthouse Academy-St. Johns, which is school #260 of 300...\n",
      "Capturing website data for Life Charter School, which is school #261 of 300...\n",
      "Capturing website data for Leadership Preparatory Academy, which is school #262 of 300...\n",
      "Capturing website data for La School For Ag Science, which is school #263 of 300...\n",
      "Capturing website data for Kipp Aspire Academy, which is school #264 of 300...\n",
      "Capturing website data for Kipp Generations Collegiate, which is school #265 of 300...\n",
      "Capturing website data for Kanu O Ka Aina - New Century Pcs, which is school #266 of 300...\n",
      "Capturing website data for Jamaa Learning Center, which is school #267 of 300...\n",
      "Capturing website data for Isucceed Virtual High School, which is school #268 of 300...\n",
      "Capturing website data for Innovations Academy, which is school #269 of 300...\n",
      "Capturing website data for Idea Charter School, which is school #270 of 300...\n",
      "Capturing website data for Harmony School Of Science - Houston, which is school #271 of 300...\n",
      "Capturing website data for Highland Free School, which is school #272 of 300...\n",
      "Capturing website data for Animo South Los Angeles Charter, which is school #273 of 300...\n",
      "Capturing website data for Gowan Science Academy, which is school #274 of 300...\n",
      "Capturing website data for Girls Preparatory Charter School Of The Bronx, which is school #275 of 300...\n",
      "Capturing website data for Gillingham Charter School, which is school #276 of 300...\n",
      "Capturing website data for Frederick Classical Charter School, which is school #277 of 300...\n",
      "Capturing website data for Joseph S. Clark Preparatory High School, which is school #278 of 300...\n",
      "Capturing website data for Arthur Ashe Charter School, which is school #279 of 300...\n",
      "Capturing website data for Excellence Boys Charter School Of Bedford Stuyvesa, which is school #280 of 300...\n",
      "Capturing website data for Evolution Academy Charter School, which is school #281 of 300...\n",
      "Capturing website data for Enlace Academy, which is school #282 of 300...\n",
      "Capturing website data for Ascend, which is school #283 of 300...\n",
      "Capturing website data for Democracy Prep Harlem Charter School, which is school #284 of 300...\n",
      "Capturing website data for Discovery Charter Preparatory No. 2, which is school #285 of 300...\n",
      "Capturing website data for Detroit Community Schools-High School, which is school #286 of 300...\n",
      "Capturing website data for Deming Cesar Chavez, which is school #287 of 300...\n",
      "Capturing website data for The Community Roots School, which is school #288 of 300...\n",
      "Capturing website data for Clear Horizons Early College H S, which is school #289 of 300...\n",
      "Capturing website data for Athlos Traditional Academy, which is school #290 of 300...\n",
      "Capturing website data for Franklin Academy F, which is school #291 of 300...\n",
      "Capturing website data for Baker Web Academy, which is school #292 of 300...\n",
      "Capturing website data for Aspire Vanguard College Preparatory Academy, which is school #293 of 300...\n",
      "Capturing website data for Aspire Eres Academy, which is school #294 of 300...\n",
      "Capturing website data for Benton County School Of Arts/Elem./Mid, which is school #295 of 300...\n",
      "Capturing website data for Plc Arts Academy At Scottsdale  Inc., which is school #296 of 300...\n",
      "Capturing website data for Woodburn Arthur Academy, which is school #297 of 300...\n",
      "Capturing website data for Arroyo Elementary School, which is school #298 of 300...\n",
      "Capturing website data for Academy Of Educational Excellence, which is school #299 of 300...\n",
      "Capturing website data for Academy Of Arts And Sciences: Thousand Oaks & Simi, which is school #300 of 300...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "k=200 # initialize this numerical variable k, which keeps track of which entry in the sample we are on.\n",
    "\n",
    "#testing the first 10 tuples\n",
    "tuple_test = tuple_list[200:300]\n",
    "\n",
    "for tup in tuple_test:\n",
    "    school_title = tup[1].title()\n",
    "    k += 1 # Add one to k, so we start with 1 and increase by 1 all the way up to entry # 300\n",
    "    print(\"Capturing website data for\", school_title + \", which is school #\" + str(k), \"of 300...\")\n",
    "    \n",
    "    # use the tuple to create a name for the folder\n",
    "    dirname = format_folder_name(k, school_title)\n",
    "    \n",
    "    run_wget_command(tup[0], wget_folder, dirname)\n",
    "    \n",
    "    school_folder = wget_folder + '/'+ dirname\n",
    "    if contains_html(school_folder):\n",
    "        write_file( tup[0], success_file )\n",
    "    else :\n",
    "        write_file( tup[0], fail_file)\n",
    "print(\"done!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation of wget\n",
    "\n",
    "-only works for static HTML and it doesnâ€™t support JavaScript. Thus any element generated by JS will not be captured. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More info:\n",
    "\n",
    "https://www.petekeen.net/archiving-websites-with-wget\n",
    "\n",
    "http://askubuntu.com/questions/411540/how-to-get-wget-to-download-exact-same-web-page-html-as-browser\n",
    "\n",
    "https://www.reddit.com/r/linuxquestions/comments/3tb7vu/wget_specify_dns_server/\n",
    "failed: nodename nor servname provided, or not known.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check(url):\n",
    "    \"\"\" Helper function, check if url is a valid list\"\"\"\n",
    "    try:\n",
    "        urlopen(url)\n",
    "        \n",
    "    except urllib.error.URLError:\n",
    "        return False\n",
    "    except urllib.error.HTTPError:\n",
    "        return False\n",
    "    except SocketError:\n",
    "        return False\n",
    "    return True\n",
    "#want to see how many html files?\n",
    "# want to see how many links are invalid?\n",
    "#read txt file\n",
    "\n",
    "def read_txt(txt_file):\n",
    "    links = []\n",
    "    count = 0\n",
    "    with open(txt_file) as f:\n",
    "        for line in f:     \n",
    "#             elem =  line.split('\\t')[1].rstrip()\n",
    "#             if elem.endswith('\\'):\n",
    "#                 elem = elem[:-1]\n",
    "            count +=1\n",
    "#             print(elem)\n",
    "            links += [line.rstrip()]\n",
    "    return links, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 243 links in success file.\n"
     ]
    }
   ],
   "source": [
    "success_links, count = read_txt(success_file)\n",
    "print(\"There are {} links in success file.\".format( count))\n",
    "# print(success_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57 links in fail file.\n"
     ]
    }
   ],
   "source": [
    "fail_links, count = read_txt(fail_file)\n",
    "print(\"There are {} links in fail file.\".format( count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# counting # of html files\n",
    "# def count_html(file):\n",
    "    \n",
    "def count_valid_links(list_of_links, valid_file, invalid_file):\n",
    "    count_success, count_fail = 0, 0\n",
    "    valid, invalid = '', ''\n",
    "    for l in list_of_links:\n",
    "#         print(l)\n",
    "        if check(l):\n",
    "            valid += l + '\\n'\n",
    "            count_success +=1\n",
    "        else:\n",
    "            invalid += l + '\\n'\n",
    "            count_fail += 1\n",
    "    write_file(valid, valid_file)\n",
    "    write_file(invalid, invalid_file)\n",
    "    return count_success, count_fail\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_list = '/Users/anhnguyen/Desktop/research/scraping_Python/valid_links.txt'\n",
    "invalid_list = '/Users/anhnguyen/Desktop/research/scraping_Python/invalid_links.txt'\n",
    "reset_text_file(valid_list)\n",
    "reset_text_file(invalid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "count_success, count_fail = count_valid_links(fail_links, valid_list, invalid_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 31 valid links and 26 invalid links\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} valid links and {} invalid links\".format(count_success, count_fail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# recheck links without \"/\"\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wget using accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os, csv\n",
    "import shutil\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "from socket import error as SocketError\n",
    "import errno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setting directories\n",
    "micro_sample_cvs = \"/Users/anhnguyen/Desktop/research/scraping_Python/micro-sample_Feb17.csv\"\n",
    "wget_folder = \"/Users/anhnguyen/Desktop/research/scraping_Python/wget_accept\"\n",
    "no_dir_folder = \"/Users/anhnguyen/Desktop/research/scraping_Python/no_dir\"\n",
    "learning_wget = \"/Users/anhnguyen/Desktop/research/scraping_Python/learning_wget\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = [] # make empty list\n",
    "with open(micro_sample_cvs, 'r', encoding = 'Windows-1252')\\\n",
    "as csvfile: # open file; the windows-1252 encoding looks weird but works for this\n",
    "    reader = csv.DictReader(csvfile) # create a reader\n",
    "    for row in reader: # loop through rows\n",
    "        sample.append(row) # append each row to the list\n",
    "        \n",
    "#note: each row, sample[i] is a dictionary with keys as column name and value as info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turning this into tuples we can use with wget!\n",
    "# first, make some empty lists\n",
    "url_list = []\n",
    "name_list = []\n",
    "terms_list = []\n",
    "\n",
    "# now let's fill these lists with content from the sample\n",
    "for school in sample:\n",
    "    url_list.append(school[\"URL\"])\n",
    "    name_list.append(school[\"SCHNAM\"])\n",
    "    terms_list.append(school[\"ADDRESS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://www.richland2.org/charterhigh/', 'RICHLAND TWO CHARTER HIGH'), ('https://www.polk.edu/lakeland-gateway-to-college-high-school/', 'POLK STATE COLLEGE COLLEGIATE HIGH SCHOOL'), ('https://www.nhaschools.com/schools/rivercity/Pages/default.aspx', 'RIVER CITY SCHOLARS CHARTER ACADEMY')]\n",
      "\n",
      " Polk State College Collegiate High School\n"
     ]
    }
   ],
   "source": [
    "tuple_list = list(zip(url_list, name_list))\n",
    "# Let's check what these tuples look like:\n",
    "print(tuple_list[:3])\n",
    "print(\"\\n\", tuple_list[1][1].title())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_parent_link(str):\n",
    "    \"\"\"Function to get parents' links. Return a list of valid links.\"\"\"\n",
    "    ls= get_parent_link_helper(5, str, []);\n",
    "    if len(ls) > 1:\n",
    "        return ls[0]\n",
    "    return str\n",
    "\n",
    "def get_parent_link_helper(level, str, result):\n",
    "    \"\"\"This is a tail recursive function\n",
    "    to get parent link of a given link. Return a list of urls \"\"\"\n",
    "    if level == 0 or not check(str):\n",
    "        return ''\n",
    "    else:\n",
    "        result += [str]\n",
    "        return get_parent_link_helper(num -1, str[: str.rindex('/')], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_folder_name (k, name):\n",
    "    \"\"\"Format a folder nicely for easy access\"\"\"\n",
    "    if k < 10: # Add two zeros to the folder name if k is less than 10 (for ease of organizing the output folders)\n",
    "        dirname = \"00\" + str(k) + \" \" + name\n",
    "    elif k < 100: # Add one zero if k is less than 100\n",
    "        dirname = \"0\" + str(k) + \" \" + name\n",
    "    else: # Add nothing if k>100\n",
    "        dirname = str(k) + \" \" + name\n",
    "    return dirname\n",
    "\n",
    "def run_wget_command(link, parent_folder, my_folder):\n",
    "    \"\"\"wget on link and print output to appropriate folders\"\"\"\n",
    "    #navigate to parent folder\n",
    "    os.chdir(parent_folder)\n",
    "    # create dir my_folder if it doesn't exist yet\n",
    "    if not os.path.exists(my_folder):\n",
    "        os.makedirs(my_folder)\n",
    "    #navigate to the correct folder, ready to wget\n",
    "    os.chdir(my_folder)\n",
    "    os.system('wget --header=\"Accept: text/html\" -r --level=3 --accept .html --referer= '+get_parent_link(link) + ' ' + link)\n",
    "#     os.system('wget -np --no-parent --show-progress --progress=dot --recursive --level=3 --convert-links --retry-connrefused \\\n",
    "#          --random-wait --no-cookies --secure-protocol=auto --no-check-certificate --execute robots=off \\\n",
    "#          --header \"Accept: text/html\" \\\n",
    "#          --user-agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\" \\\n",
    "#           --accept .html' + ' ' + link)\n",
    "    \n",
    "\n",
    "def contains_html(my_folder):\n",
    "    \"\"\"check if a wget is success by checking if a directory has a html file\"\"\"\n",
    "\n",
    "    for r,d,f in os.walk(my_folder):\n",
    "        for file in f:\n",
    "            if file.endswith('.html'):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def count_with_file_ext(folder, ext):\n",
    "    count = 0\n",
    "    for r,d,f in os.walk(my_folder):\n",
    "        for file in f:\n",
    "            if file.endswith(ext):\n",
    "                count +=1\n",
    "    return count \n",
    "\n",
    "# write a file and add num line at the beginning of line\n",
    "def write_to_file(num, link, file_name):\n",
    "    with open(file_name, \"a\") as text_file:\n",
    "        text_file.write(str(num) + \"\\t\" + link +\"\\n\")\n",
    "\n",
    "# just write str to file\n",
    "def write_file(str, file_name):\n",
    "    with open(file_name, \"a\") as text_file:\n",
    "        text_file.write(str)\n",
    "        \n",
    "def reset(folder, text_file_1, text_file_2):\n",
    "    \"\"\"Deletes all files in a folder and set 2 text files to blank\"\"\"\n",
    "    parent_folder = folder[: folder.rindex('/')]\n",
    "    shutil.rmtree(folder)\n",
    "    os.makedirs(folder)\n",
    "    filelist = [ f for f in os.listdir(folder) if f.endswith(\".bak\") ]\n",
    "    for f in filelist:\n",
    "        os.unlink(f)\n",
    "    for file_name in [text_file_1, text_file_2]:\n",
    "        reset_text_file(file_name)\n",
    "        \n",
    "def reset_text_file(file_name):\n",
    "    if os.path.exists(file_name):\n",
    "            with open(file_name, \"w\") as text_file:\n",
    "                text_file.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "030 name me\n"
     ]
    }
   ],
   "source": [
    "#testing methods\n",
    "print(format_folder_name(30, \"name me\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(url):\n",
    "    \"\"\" Helper function, check if url is a valid list\"\"\"\n",
    "    try:\n",
    "        urlopen(url)\n",
    "        \n",
    "    except urllib.error.URLError:\n",
    "        print(\"urllib.error.URLError\")\n",
    "        return False\n",
    "    except urllib.error.HTTPError:\n",
    "        print('urllib.error.HTTPError')\n",
    "        return False\n",
    "    except SocketError:\n",
    "        print('SocketError')\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def read_txt(txt_file):\n",
    "    links = []\n",
    "    count = 0\n",
    "    with open(txt_file) as f:\n",
    "        for line in f:   \n",
    "            \n",
    "            elem =  line.split('\\t')[1].rstrip()\n",
    "            count +=1\n",
    "    \n",
    "#             print(elem)\n",
    "            links += [elem.rstrip()]\n",
    "    return links, count\n",
    "\n",
    "def read_txt_2(txt_file):\n",
    "    links = []\n",
    "    count = 0\n",
    "    with open(txt_file) as f:\n",
    "        for line in f:   \n",
    "            \n",
    "#             elem =  line.split('\\t')[1].rstrip()\n",
    "#             if elem.endswith('\\'):\n",
    "#                 elem = elem[:-1]\n",
    "            count +=1\n",
    "    \n",
    "#             print(elem)\n",
    "            links += [line.rstrip()]\n",
    "    return links, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up file directories\n",
    "success_file = \"/Users/anhnguyen/Desktop/research/scraping_Python/success.txt\"\n",
    "fail_file = \"/Users/anhnguyen/Desktop/research/scraping_Python/fail.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_now = '/Users/anhnguyen/Desktop/research/scraping_Python/validlinks_from_Sammy.txt'\n",
    "list_valid_now,count = read_txt_2(valid_now)\n",
    "for link in list_valid_now:\n",
    "    run_wget_command(str(link), wget_folder, \"new \"+ str(link)[6:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset(wget_folder, success_file, fail_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k=200 # initialize this numerical variable k, which keeps track of which entry in the sample we are on.\n",
    "\n",
    "#testing the first 10 tuples\n",
    "# tuple_test = tuple_list[200:300]\n",
    "\n",
    "\n",
    "for tup in tuple_test:\n",
    "    school_title = tup[1].title()\n",
    "\n",
    "\n",
    "    k += 1 # Add one to k, so we start with 1 and increase by 1 all the way up to entry # 300\n",
    "    print(\"Capturing website data for\", school_title + \", which is school #\" + str(k), \"of 300...\")\n",
    "    \n",
    "    # use the tuple to create a name for the folder\n",
    "    dirname = format_folder_name(k, school_title)\n",
    "    \n",
    "    run_wget_command(tup[0], wget_folder, dirname)\n",
    "    \n",
    "    school_folder = wget_folder + '/'+ dirname\n",
    "    if contains_html(school_folder):\n",
    "        write_file( tup[0], success_file )\n",
    "    else :\n",
    "        write_file( tup[0], fail_file)\n",
    "print(\"done!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation of wget\n",
    "\n",
    "-only works for static HTML and it doesnâ€™t support JavaScript. Thus any element generated by JS will not be captured. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More info:\n",
    "\n",
    "https://www.petekeen.net/archiving-websites-with-wget\n",
    "\n",
    "http://askubuntu.com/questions/411540/how-to-get-wget-to-download-exact-same-web-page-html-as-browser\n",
    "\n",
    "https://www.reddit.com/r/linuxquestions/comments/3tb7vu/wget_specify_dns_server/\n",
    "failed: nodename nor servname provided, or not known.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 243 links in success file.\n"
     ]
    }
   ],
   "source": [
    "success_links, count = read_txt(success_file)\n",
    "print(\"There are {} links in success file.\".format( count))\n",
    "# print(success_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57 links in fail file.\n"
     ]
    }
   ],
   "source": [
    "fail_links, count = read_txt(fail_file)\n",
    "print(\"There are {} links in fail file.\".format( count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting # of html files\n",
    "# def count_html(file):\n",
    "    \n",
    "def count_valid_links(list_of_links, valid_file, invalid_file):\n",
    "    count_success, count_fail = 0, 0\n",
    "    valid, invalid = '', ''\n",
    "    for l in list_of_links:\n",
    "#         print(l)\n",
    "        if check(l):\n",
    "            valid += l + '\\n'\n",
    "            count_success +=1\n",
    "        else:\n",
    "            invalid += l + '\\n'\n",
    "            count_fail += 1\n",
    "#             print(l)\n",
    "    write_file(valid, valid_file)\n",
    "    write_file(invalid, invalid_file)\n",
    "    return count_success, count_fail\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_list = '/Users/anhnguyen/Desktop/research/scraping_Python/valid_links.txt'\n",
    "invalid_list = '/Users/anhnguyen/Desktop/research/scraping_Python/invalid_links.txt'\n",
    "reset_text_file(valid_list)\n",
    "reset_text_file(invalid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n",
      "urllib.error.URLError\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count_success, count_fail = count_valid_links(fail_links, valid_list, invalid_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 31 valid links and 26 invalid links\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} valid links and {} invalid links\".format(count_success, count_fail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "# recheck links without \"/\"\n",
    "recheck, count = read_txt_2(invalid_list)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://responsiveed.com/dallasclassical\n"
     ]
    }
   ],
   "source": [
    "for index in range (0, len(recheck)):\n",
    "    if recheck[index].endswith('/'):\n",
    "        recheck[index] = recheck[index][: recheck[index].rindex('/')]\n",
    "print(recheck[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.trinityschoolforchildren.org\n",
      "http://www.pasadenarosebud.com\n",
      "http://www.mlacademy.org/#!contact-us/c2q4\n",
      "http://www.materacademy.com/schools\n",
      "http://www.jeffersoncommunityschool.org\n",
      "http://www.evergladesprep.com/pages/Everglades_Preparatory_Academy\n",
      "http://www.clevelandta.org/school/oak-leadership-institute\n",
      "http://www.chandlerparkacademy.net/index.php/schools/elementary-school.html\n",
      "http://www.ccaschool.net\n",
      "http://www.blracademy.org\n",
      "http://www.academycharterhs.org/pages/mainpg\n",
      "http://www.academiadeestrellas.org\n",
      "http://rpes-susd-ca.schoolloop.com\n",
      "http://responsiveed.com/premierpharrmcallen\n",
      "http://responsiveed.com/premiernewbraunfels\n",
      "http://responsiveed.com/huntsvilleclassical\n",
      "http://responsiveed.com/dallasclassical\n",
      "http://ideacharterschool.com\n",
      "http://gowan.craneschools.org\n",
      "http://arthuracademy.org/woodburn/woodburn-arthur-academy.html\n"
     ]
    }
   ],
   "source": [
    "invalid2 = '/Users/anhnguyen/Desktop/research/scraping_Python/invalid2.txt'\n",
    "count_success, count_fail = count_valid_links(recheck, valid_list, invalid2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 valid links and 20 invalid links\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} valid links and {} invalid links\".format(count_success, count_fail))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Runing wget with log output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting up files\n",
    "invalid2 = '/Users/anhnguyen/Desktop/research/scraping_Python/invalid2.txt'\n",
    "log = '/Users/anhnguyen/Desktop/research/scraping_Python/wget_accept_logs.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "failed_links, counts = read_txt(invalid2)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urllib.error.URLError\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "## something wrong with check function???\n",
    "print(check('http://responsiveed.com/dallasclassical'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/anhnguyen/Desktop/research/scraping_Python/no_dir')\n",
    "reset_text_file(log)\n",
    "for link in failed_links:\n",
    "    \n",
    "    \n",
    "    os.system('wget -np --no-parent --show-progress --progress=dot --recursive --level=3 --convert-links --retry-connrefused --tries=5\\\n",
    "         --random-wait --no-cookies --secure-protocol=auto --no-check-certificate --execute robots=off \\\n",
    "         --header \"Host: jrs-s.net\" \\\n",
    "         --output-file=log \\\n",
    "         --user-agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\" \\\n",
    "          --accept .html' + ' ' + link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
